%% Linear model calibration:
% comparison of methods
% - OLS
% - UQLab
% - Implicit quadrature rule

clc
close all
clearvars

rng default

% select which methods to run
runOLS = 1; % Ordinary Least Squares
runUQLab = 1; % UQLab
runIQR = 1; % Implicit quadrature rule
runSampling = 1; % Brute-force sampling of prior 

%% input settings
p = 2; % dimension of parameters to be calibrated
N_data = 10; % dimension of data vector

dom = [0,2]; % domain of x

%% plot settings
fontsize = 14;
fontname = 'Helvetica';
set(0,'defaultlinelinewidth',2)

%% generate data and exact solution

% generate artificial measurement data by using the linear model
% with Gaussian noise on top of it

x_data = linspace(dom(1),dom(2),N_data)';

% stdev in measurement error - also used for likelihood in Bayes
sigma = 0.1;

% exact value for beta is only used to generate artificial measurement data, and to
% plot exact solution
beta_exact = [0.4;0.6]; % should have length p

% measurement data
z_data  = linearmodel(beta_exact,x_data) + sigma*randn(N_data,1);

% generate exact solution for plotting purposes
N_exact = 100;
x_exact = linspace(dom(1),dom(2),N_exact)';
y_exact = linearmodel(beta_exact,x_exact);

% exact solution for Bayes:
% - multivariate normal likelihood, with mean = 0 and covariance sigma^2 * I
% - uniform prior
% the posterior MAP estimate should then bethe same as the OLS estimate

% we now create the design matrix, built with the x_data points
% we create it automatically by calling linearmodel several times,
% each time with only one of the betas active
p = length(beta_exact); 
A = zeros(N_data,p); % size of design matrix, normally N_data>p
beta_test = eye(p,p);
for i=1:p
    A(:,i) = linearmodel(beta_test(:,i),x_data);
end

figure(1)
set(gcf,'DefaultAxesFontSize',fontsize,'DefaultAxesFontName',fontname);
plot(x_data,z_data,'x');
hold on
plot(x_exact,y_exact,'-');
legend('data','exact solution for generating synthetic data');

%% ordinary least squares (OLS) solution
if (runOLS==1)

    % solve the (overdetermined) least-squares problem with the regress command
    % this gives the estimate for the beta parameters
    beta_OLS = regress(z_data,A)
    % note that this is the same as
    % beta_OLS = (X' * X)\(X' * y_data)
    % and also the same as
    % beta_OLS = pinv(X)*y_data
    
    % with this estimate of beta we get the following solution
    y_OLS = linearmodel(beta_OLS,x_data);
    % alternatively, we can use linearmodel_vectorized, which will return the
    % OLS solution at the data points
    % y_OLS = linearmodel_vectorized(beta_OLS,A);
    
    % note: sigma_OLS = sqrt(sum((y_OLS - z_data).^2)/(N_data-p))
    
    figure(1)
    hold on
    plot(x_data,y_OLS,'s-');
    legappend('OLS')
end

%% Bayesian solution with UQLab
if (runUQLab == 1)
    
    %% initialize UQlab
    
    % add path
    addpath(genpath('../../../UQLabCore_Rel1.3.0/'));
    % start uqlab
    uqlab
    
    % model settings
    ModelOpts.mFile = 'linearmodel_vectorized';
    % pass design matrix as parameter to the M-file
    ModelOpts.Parameters   = A;
    ModelOpts.isVectorized = true;
    myForwardModel = uq_createModel(ModelOpts);
    
    % prior
    PriorOpts.Marginals(1).Name = 'beta0';
    PriorOpts.Marginals(1).Type = 'Uniform';
    PriorOpts.Marginals(1).Parameters = [0,1];
    %
    PriorOpts.Marginals(2).Name = 'beta1';
    PriorOpts.Marginals(2).Type = 'Uniform';
    PriorOpts.Marginals(2).Parameters = [0,1];
    %
    myPriorDist = uq_createInput(PriorOpts);
    
    
    % display input properties
    % uq_print(myPriorDist);
    % uq_display(myPriorDist);
    
    
    %% likelihood
    DiscrepancyOptsKnown.Type = 'Gaussian';
    DiscrepancyOptsKnown.Parameters = sigma^2; % this is sigma^2
    
    %% Bayes options
    Solver.Type = 'MCMC';
    % Adaptive Metropolis:
    Solver.MCMC.Sampler = 'AM';
    Solver.MCMC.Steps = 1e3;
    Solver.MCMC.NChains = 1e2;
    Solver.MCMC.T0 = 1e2;
%     Solver.MCMC.Proposal.PriorScale = 0.1;
    % AIES:
%     Solver.MCMC.Sampler = 'AIES';

    % show MCMC chain convergence:
%     Solver.MCMC.Visualize.Parameters = 1;
%     Solver.MCMC.Visualize.Interval = 100;
    
    myData.y       = z_data'; % note: UQLab uses a row vector here
    myData.Name    = 'measurement data';
    BayesOpts.Data = myData;
    BayesOpts.Type = 'Inversion';
    BayesOpts.Discrepancy = DiscrepancyOptsKnown;
    BayesOpts.Solver = Solver;
    
    %% perform MCMC
    myBayesianAnalysis = uq_createAnalysis(BayesOpts);
    
    %% postprocessing
    % text output of Bayesian analysis
    uq_print(myBayesianAnalysis)
    % graphical display of posterior
    uq_display(myBayesianAnalysis)
    
    % All post-processing results generated by the uq_postProcessInversion function are stored
    % in the myBayesianAnalysis.Results.PostProc structure.
    
    % get the Maximum a Posteriori value
    uq_postProcessInversion(myBayesianAnalysis,'pointestimate','Mean');
    beta_UQLab_mean = myBayesianAnalysis.Results.PostProc.PointEstimate.X
    % get the mean posterior value
    uq_postProcessInversion(myBayesianAnalysis,'pointEstimate', 'MAP');
    beta_UQLab_MAP = myBayesianAnalysis.Results.PostProc.PointEstimate.X
    
    y_UQLab_MAP = linearmodel(beta_UQLab_MAP',x_exact);
    y_UQLab_mu  = myBayesianAnalysis.Results.PostProc.PostPred.model.pointEstimateRun; % posterior predictive
    
    figure(1)
    hold on
    plot(x_data,y_UQLab_mu,'-.');
    legappend('UQLab posterior predictive (mean)')

end

%% Implicit Quadrature rule
if (runIQR == 1)

    addpath(genpath('/Users/sanderse/Dropbox/work/students/Laurent van den Bos/code/software/'));
    N_IQR = 20; % Number of iterations
    S_IQR = 500; % number of samples per iteration
    [nodes_IQR, w_IQR, err, n, model_evals] = calibrate_linear(@linearmodel_vectorized,'nz',N_data,...
        'z',z_data',...
        'd',p,'sigma',sigma,'A',A,'N',N_IQR,'S',S_IQR,...
        'IQR_type','C++','progress',true);
    %
    f = quadmarginals(nodes_IQR, w_IQR); % f is the figure handle
    % mean of posterior
    beta_IQR_mean = w_IQR'*nodes_IQR
    
    % posterior predictive
    % int u(x) post(beta|z) dbeta
    % expectation of the model with the posterior parameters, at the data
    % point locations
%     y_quad = linearmodel(nodes_IQR',x_data); % this contains the model evaluations as a function of beta and x
    y_quad = linearmodel_vectorized(nodes_IQR,A);
    y_quad_mu = w_IQR'*y_quad;
    y_quad_sigma =  sqrt(w_IQR'*(y_quad.^2) - y_quad_mu.^2);
    
    figure(1)
    hold on
    errorbar(x_data,y_quad_mu,y_quad_sigma,'x-','LineWidth',2)
    legappend('IQR posterior predictive (mean +- sigma)')
end


%% Chebyshev sampling of prior
if (runSampling == 1)
    addpath('/Users/sanderse/Dropbox/work/Programming/libs/');
    addpath(genpath('/Users/sanderse/Dropbox/work/Programming/libs/chebfun-master/'));
    
    % uniform sampling of prior parameter space:
    % assume domain [0,1] for both parameters
    Nbeta1 = 100;
    Nbeta2 = 100;
    beta1_sample = linspace(0,1,Nbeta1)'; %chebpts(Nbeta1,[0,1]);
    beta2_sample = linspace(0,1,Nbeta2)'; %chebpts(Nbeta2,[0,1]);
    L = zeros(Nbeta1,Nbeta2); % likelihood
    for i=1:Nbeta1
        for j=1:Nbeta2
            beta_ij = [beta1_sample(i);beta2_sample(j)];
            L(i,j) = exp( - (z_data - linearmodel(beta_ij,x_data))'*(z_data - linearmodel(beta_ij,x_data)) / (2*sigma^2) );
        end
    end
%      contourf(beta1_sample,beta2_sample,L');
%     [~,ind]=max2d(L)
%     [beta1_sample(ind(1));beta2_sample(ind(2))]
    
    % posterior is directly proportional to likelihood because we use a uniform prior
    % MAP is found by looking for maximum of L
    
    
    % alternatively, we can construct a chebfun on the likelihood directly
    L_cheb = chebfun2( @(beta1,beta2) exp( - (z_data - A*[beta1;beta2])'*(z_data - A*[beta1;beta2]) / (2*sigma^2) ), [0 1 0 1], 'vectorize');
    % and find its maximum:
    [L_max, beta_sampling_max] = max2(L_cheb);
    % or minimize the loglikelihood (z_data - A*[beta1;beta2])'*(z_data - A*[beta1;beta2]) / (2*sigma^2) 
    beta_sampling_max
    figure
    contourf(L_cheb)
end

%%
figure(1)
grid on
xlabel('x')
ylabel('y')

